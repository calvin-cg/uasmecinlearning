{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b1135d388471>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Extra\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtopology\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLeakyReLU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "# System Modules\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Extra\n",
    "from keras.engine.topology import Input\n",
    "from keras.engine.training import Model\n",
    "from keras.layers import LeakyReLU, Concatenate, Dropout\n",
    "from keras.layers.convolutional import Conv2D, UpSampling2D, Conv2DTranspose\n",
    "from keras.layers.core import Activation, SpatialDropout2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from models.utils.instance_normalization import InstanceNormalization\n",
    "from models.utils.sn import ConvSN2D\n",
    "from models.utils.calc_output_and_feature_size import calc_output_and_feature_size\n",
    "from models.utils.attention import Attention\n",
    "from keras.layers import Conv2D, Lambda, add, AvgPool2D, Activation, UpSampling2D, Input, concatenate, Reshape, LeakyReLU, Reshape, Flatten, concatenate\n",
    "\n",
    "# Custom Libs\n",
    "from models.utils.calc_output_and_feature_size import calc_output_and_feature_size\n",
    "from lib.data_utils import save_sample_images, write_log, generate_training_images\n",
    "from lib.data_utils import generator, generate_label_data\n",
    "\n",
    "# Keras Modules\n",
    "import keras\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.layers import Lambda, UpSampling2D, Input, concatenate\n",
    "from keras.utils.data_utils import  GeneratorEnqueuer\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, save_model, load_model\n",
    "from keras import backend as K\n",
    "K.clear_session()\n",
    "\n",
    "# Import models\n",
    "from models.discriminator_full import DiscriminatorFull\n",
    "from models.discriminator_low import DiscriminatorLow\n",
    "from models.discriminator_medium import DiscriminatorMedium\n",
    "from models.core_generator import CoreGenerator\n",
    "\n",
    "# Other Modules \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# ----------\n",
    "#  Settings \n",
    "# ----------\n",
    "\n",
    "height = 128\n",
    "width = 128\n",
    "channels = 1\n",
    "epochs = 10\n",
    "gpus = 1\n",
    "batch_size = 5\n",
    "cpus = 2\n",
    "use_multiprocessing = True\n",
    "save_weights_every_n_epochs = 0.01\n",
    "max_queue_size=batch_size * 1\n",
    "img_dir = \"./Train/\"\n",
    "test_dir = \"./Test/\"\n",
    "resource_dir = \"./resources/\"\n",
    "dataset_len = len(os.listdir(img_dir))\n",
    "testset_len = len(os.listdir(test_dir))\n",
    "learning_rate = 0.0002\n",
    "experiment_name = time.strftime(\"%Y-%m-%d-%H-%M\")\n",
    "decay_rate = 0\n",
    "decay_rate = learning_rate / ((dataset_len / batch_size) * epochs)\n",
    "\n",
    "\n",
    "# ----------------------------------\n",
    "# Load filenames\n",
    "#-----------------------------------\n",
    "\n",
    "X = []\n",
    "for filename in os.listdir(img_dir):\n",
    "    X.append(filename)\n",
    "\n",
    "Test = []\n",
    "for filename in os.listdir(test_dir):\n",
    "    Test.append(filename)    \n",
    "    \n",
    "# ----------------------------------\n",
    "#  Create directory for sample data\n",
    "# ----------------------------------\n",
    "\n",
    "main_dir = './output/256/' + experiment_name\n",
    "save_sample_images_dir = main_dir + '/sample_images/'\n",
    "save_validation_images_dir = main_dir + '/validation_images/'\n",
    "weights_dir = main_dir +'/weights/'\n",
    "log_path = main_dir + '/logs/'\n",
    "model_path = main_dir + '/models/'\n",
    "\n",
    "if not os.path.exists(main_dir):\n",
    "    os.makedirs(main_dir)\n",
    "    os.makedirs(save_sample_images_dir)\n",
    "    os.makedirs(save_validation_images_dir)\n",
    "    os.makedirs(log_path)\n",
    "    os.makedirs(weights_dir)\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "# ---------------\n",
    "#  Import Models \n",
    "# ---------------\n",
    "    \n",
    "core_generator = CoreGenerator(gpus=gpus, width=width, height=height)\n",
    "discriminator_full = DiscriminatorFull(gpus=gpus, decay_rate=decay_rate, width=width, height=height)\n",
    "discriminator_medium = DiscriminatorMedium(gpus=gpus, decay_rate=decay_rate, width=width, height=height)\n",
    "discriminator_low = DiscriminatorLow(gpus=gpus, decay_rate=decay_rate, width=width, height=height)\n",
    "\n",
    "if os.path.isdir(\"./resources/\"):\n",
    "    core_generator.model.load_weights('./resources/core_generator.h5')\n",
    "    discriminator_full.model.load_weights('./resources/discriminator_full.h5')\n",
    "    discriminator_medium.model.load_weights('./resources/discriminator_medium.h5')\n",
    "    discriminator_low.model.load_weights('./resources/discriminator_low.h5')\n",
    "\n",
    "# Create a directory to save weights\n",
    "if not os.path.exists(resource_dir):\n",
    "    os.makedirs(resource_dir)\n",
    "\n",
    "discriminator_full.trainable = False\n",
    "discriminator_medium.model.trainable = False\n",
    "discriminator_full.model.trainable = False\n",
    "\n",
    "\n",
    "# --------------------------------\n",
    "#  Create GAN with core generator\n",
    "# --------------------------------\n",
    "\n",
    "# Generate image with core generator\n",
    "gan_x = Input(shape=(height, width, channels,))\n",
    "gan_y = Input(shape=(height, width, 2,))\n",
    "\n",
    "# Extract style features and add them to image\n",
    "gan_output = core_generator.model(gan_x)\n",
    "\n",
    "# Extract features and predictions from discriminators\n",
    "disc_input = concatenate([gan_x, gan_output], axis=-1)\n",
    "pred_full, features_full = discriminator_full.model(disc_input)\n",
    "pred_medium, features_medium = discriminator_medium.model(disc_input)\n",
    "pred_low, features_low = discriminator_low.model(disc_input)\n",
    "\n",
    "# Compile GAN\n",
    "gan_core = Model(inputs=gan_x, outputs=[gan_output, features_full, features_medium, features_low, pred_full, pred_medium, pred_low])                  \n",
    "\n",
    "gan_core.name = \"gan_core\"\n",
    "optimizer = Adam(learning_rate, 0.5, decay=decay_rate)\n",
    "loss_gan = ['mae', 'mae', 'mae', 'mae', 'mse', 'mse', 'mse']\n",
    "loss_weights_gan = [1, 3.33, 3.33, 3.33, 0.33, 0.33, 0.33]\n",
    "\n",
    "# gan_core = multi_gpu_model(gan_core_org)\n",
    "gan_core.compile(optimizer=optimizer, loss_weights=loss_weights_gan, loss=loss_gan)\n",
    "\n",
    "\n",
    "# --------------------------------\n",
    "#  Compile Discriminator\n",
    "# --------------------------------\n",
    "\n",
    "discriminator_full.model.trainable = True\n",
    "discriminator_medium.model.trainable = True\n",
    "discriminator_low.model.trainable = True\n",
    "\n",
    "def zero_loss(y_true, y_pred):\n",
    "    return K.zeros_like(y_true)\n",
    "\n",
    "loss_d = ['mse', zero_loss]\n",
    "loss_weights_d = [1, 0]\n",
    "optimizer_dis = Adam(learning_rate, 0.5, decay=decay_rate)\n",
    "\n",
    "discriminator_full_multi = discriminator_full.model\n",
    "discriminator_medium_multi = discriminator_medium.model\n",
    "discriminator_low_multi = discriminator_low.model\n",
    "\n",
    "discriminator_full_multi.compile(optimizer=optimizer_dis, loss_weights=loss_weights_d, loss=loss_d)\n",
    "discriminator_medium_multi.compile(optimizer=optimizer_dis, loss_weights=loss_weights_d, loss=loss_d)\n",
    "discriminator_low_multi.compile(optimizer=optimizer_dis, loss_weights=loss_weights_d, loss=loss_d)\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "#  Initiate Generator Queue\n",
    "# --------------------------------------------------\n",
    "\n",
    "enqueuer = GeneratorEnqueuer(generator(X, img_dir, batch_size, dataset_len, width, height), use_multiprocessing=use_multiprocessing, wait_time=0.01)\n",
    "\n",
    "enqueuer.start(workers=cpus, max_queue_size=max_queue_size)\n",
    "output_generator = enqueuer.get()\n",
    "\n",
    "# ---------------------------------\n",
    "#  Initiate values for Tensorboard\n",
    "# ---------------------------------\n",
    "\n",
    "callback_Full = TensorBoard(log_path)\n",
    "callback_Medium = TensorBoard(log_path)\n",
    "callback_Low = TensorBoard(log_path)\n",
    "callback_gan = TensorBoard(log_path)\n",
    "\n",
    "callback_Full.set_model(discriminator_full.model)\n",
    "callback_Medium.set_model(discriminator_medium.model)\n",
    "callback_Low.set_model(discriminator_low.model)\n",
    "callback_gan.set_model(gan_core)\n",
    "\n",
    "callback_Full_names = ['weighted_loss_real_full', 'disc_loss_real_full', 'zero_1', 'weighted_loss_fake_full', 'disc_loss_fake_full', 'zero_2']\n",
    "callback_Medium_names = ['weighted_loss_real_low', 'disc_loss_real_medium', 'zero_3', 'weighted_loss_fake_medium', 'disc_loss_fake_medium', 'zero_4']\n",
    "callback_Low_names = ['weighted_loss_real_low', 'disc_loss_real_low', 'zero_3', 'weighted_loss_fake_low', 'disc_loss_fake_low', 'zero_4']\n",
    "callback_gan_names = ['total_gan_loss', 'image_diff', 'feature_diff_disc_full', 'feature_diff_disc_low', 'predictions_full', 'predictions_low']\n",
    "\n",
    "# Decide how often to create sample images, save log data, and weights. \n",
    "cycles = int(epochs * (dataset_len / batch_size))\n",
    "save_images_cycle = int((dataset_len / batch_size))\n",
    "save_weights_cycle = int((dataset_len / batch_size))\n",
    "\n",
    "# Calculate the discriminator output size for features and image predictions\n",
    "pred_size_f, feat_size_f = calc_output_and_feature_size(width, height)\n",
    "pred_size_m, feat_size_m = calc_output_and_feature_size(width/2, height/2)\n",
    "pred_size_l, feat_size_l = calc_output_and_feature_size(width/4, height/4)\n",
    "\n",
    "# Create benchmark to see progress\n",
    "start = time.time()\n",
    "\n",
    "def concatenateNumba(x, y):\n",
    "    return np.concatenate([x, y], axis=-1)\n",
    "\n",
    "for i in range(0, cycles):\n",
    "    start_c = time.time()\n",
    "    # ------------------------\n",
    "    #  Generate Training Data\n",
    "    # ------------------------\n",
    "\n",
    "    # Discriminator data\n",
    "    x_full, y_full, x_and_y_full = next(output_generator)\n",
    "    x_medium, y_medium, x_and_y_medium = next(output_generator)\n",
    "    x_low, y_low, x_and_y_low = next(output_generator)\n",
    "    \n",
    "    # Fixed data\n",
    "    fake_labels_f, true_labels_f, dummy_f = generate_label_data(batch_size, pred_size_f, feat_size_f)\n",
    "    fake_labels_m, true_labels_m, dummy_m = generate_label_data(batch_size, pred_size_m, feat_size_m)\n",
    "    fake_labels_l, true_labels_l, dummy_l = generate_label_data(batch_size, pred_size_l, feat_size_l)\n",
    "  \n",
    "    # GAN data\n",
    "    x_gan, y_gan, x_and_y_gan = next(output_generator)\n",
    "\n",
    "    # ----------------------\n",
    "    #  Train Discriminators \n",
    "    # ----------------------\n",
    "\n",
    "    y_gen_full, _, _, _, _, _, _ = gan_core.predict(x_full)\n",
    "    x_and_y_gen_full = concatenateNumba(x_full, y_gen_full)\n",
    "    \n",
    "    # Prepare data for Medium Resolution Discriminator \n",
    "    y_gen_medium, _, _, _, _ , _, _= gan_core.predict(x_medium)\n",
    "    x_and_y_gen_medium = concatenateNumba(x_medium, y_gen_medium)\n",
    "    \n",
    "    # Prepare data for Low Resolution Discriminator \n",
    "    y_gen_low, _, _, _, _ , _, _= gan_core.predict(x_low)\n",
    "    x_and_y_gen_low = concatenateNumba(x_low, y_gen_low)\n",
    "\n",
    "    # Train Discriminators \n",
    "    d_loss_fake_full = discriminator_full_multi.train_on_batch(x_and_y_gen_full, [fake_labels_f, dummy_f])\n",
    "    d_loss_real_full = discriminator_full_multi.train_on_batch(x_and_y_full, [true_labels_f, dummy_f])\n",
    "    \n",
    "    d_loss_fake_medium = discriminator_medium_multi.train_on_batch(x_and_y_gen_medium, [fake_labels_m, dummy_m])\n",
    "    d_loss_real_medium = discriminator_medium_multi.train_on_batch(x_and_y_medium, [true_labels_m, dummy_m])\n",
    "   \n",
    "    d_loss_fake_low = discriminator_low_multi.train_on_batch(x_and_y_gen_low, [fake_labels_l, dummy_l])\n",
    "    d_loss_real_low = discriminator_low_multi.train_on_batch(x_and_y_low, [true_labels_l, dummy_l])\n",
    "\n",
    "    # -----------\n",
    "    #  Train GAN\n",
    "    # -----------\n",
    "    \n",
    "\n",
    "    # Extract featuers from discriminators \n",
    "    _, real_features_full = discriminator_full_multi.predict(x_and_y_gan)\n",
    "    _, real_features_medium = discriminator_medium_multi.predict(x_and_y_gan)\n",
    "    _, real_features_low = discriminator_low_multi.predict(x_and_y_gan)\n",
    "    \n",
    "    # Train GAN on one batch\n",
    "    gan_core_loss = gan_core.train_on_batch(x_gan, [y_gan, \n",
    "                                                    real_features_full,\n",
    "                                                    real_features_medium,\n",
    "                                                    real_features_low,\n",
    "                                                    true_labels_f,\n",
    "                                                    true_labels_m,\n",
    "                                                    true_labels_l])\n",
    "\n",
    "    # -------------------------------------------\n",
    "    #  Save image samples, weights, and log data\n",
    "    # -------------------------------------------\n",
    "    \n",
    "    # Print log data to tensorboard\n",
    "    write_log(callback_Full, callback_Full_names, d_loss_fake_full + d_loss_real_full, i)\n",
    "    write_log(callback_Medium, callback_Medium_names, d_loss_fake_medium + d_loss_real_medium, i)\n",
    "    write_log(callback_Low, callback_Low_names, d_loss_fake_low + d_loss_real_low, i)\n",
    "    write_log(callback_gan, callback_gan_names, gan_core_loss, i)\n",
    "    \n",
    "    end_c = time.time()\n",
    "    print(\"\\n\\nCycle:\", i)\n",
    "    print(\"Time:\", end_c - start_c)\n",
    "    print(\"Total images:\", batch_size * i)\n",
    "\n",
    "    # Save sample images\n",
    "    if i % save_images_cycle == 0:\n",
    "        print('Print those bad boys:', i)\n",
    "        end = time.time()\n",
    "        hours, rem = divmod(end-start, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "        print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "        x_val, y_val, x_y_val = generate_training_images(Test, 5, testset_len, width, height, test_dir)\n",
    "        output_benchmark, _, _, _, _, _ ,_ = gan_core.predict(x_val)\n",
    "        save_sample_images(output_benchmark, x_val, 'b-' + str(i), save_validation_images_dir)\n",
    "        save_sample_images(y_gen_full, x_full, str(i), save_sample_images_dir)\n",
    "        start = time.time()\n",
    "\n",
    "    #  Save weights\n",
    "    if i % save_weights_cycle == 0:\n",
    "        discriminator_full.model.save_weights(weights_dir + str(i) + \"-discriminator_full.h5\")\n",
    "        discriminator_medium.model.save_weights(weights_dir + str(i) + \"-discriminator_medium.h5\")\n",
    "        discriminator_low.model.save_weights(weights_dir + str(i) + \"-discriminator_low.h5\")\n",
    "        core_generator.model.save_weights(weights_dir + str(i) + \"-core_generator.h5\")\n",
    "\n",
    "        \n",
    "        discriminator_full.model.save_weights(resource_dir + \"discriminator_full.h5\")\n",
    "        discriminator_medium.model.save_weights(resource_dir + \"discriminator_medium.h5\")\n",
    "        discriminator_low.model.save_weights(resource_dir + \"discriminator_low.h5\")\n",
    "        core_generator.model.save_weights(resource_dir + \"core_generator.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
